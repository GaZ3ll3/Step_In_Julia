{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:b6c2be538ef6c793379a7b8fafc0f2ab7acb6f62ca7f70386b48f39716aa31a3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Machine Representation of Numbers ##\n",
      "\n",
      "``Julia`` supports multiple float-point types, including ``Float16`` (half), ``Float32`` (single) and  ``Float64`` (double). Half-precision floating-point numbers are only for storage format, when ``Float16`` type is involved in computation, it will be automatically promoted into ``Float32``. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@printf(\"size of Float64 1.0 is %d.\\n\", sizeof(1.0))\n",
      "@printf(\"size of Float32 1.0 is %d.\\n\", sizeof(float32(1.0)))\n",
      "@printf(\"size of Float16 1.0 is %d.\\n\", sizeof(float16(1.0)))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "``NaN`` and ``Inf`` (as well as ``-Inf``) are special floating-points(IEC559), and can be cast into all floating-point types also be used in arithmetic operations.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@printf(\"Inf * Inf + Inf = %d\\n\", Inf * Inf + Inf) \n",
      "@printf(\"Inf + (-Inf)    = %d\\n\", Inf + (-Inf))\n",
      "@printf(\"Inf * Inf + Inf = %lf\\n\", Inf * Inf + Inf) \n",
      "@printf(\"Inf + (-Inf)    = %lf\\n\", Inf + (-Inf))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Epsilon function ``eps`` gives machine accuracy. For single and double accuracy, epsilon will be ``float32(2.0^-23)`` and ``2.0^-53`` relatively. Notice ``eps`` also can be used on ``Float16``, the result will be also a half precision number.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@printf(\"%4.52lf\\n\", eps(Float16))\n",
      "@printf(\"%4.52lf\\n\" ,eps(Float32))\n",
      "@printf(\"%4.52lf\\n\", eps(Float64))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The default rounding mode is ``RoundNearest``, to change the mode, we can use ``with_rounding``. In the following example, ``1.2000000000000001`` cannot be represented, thus ``Julia`` rounds it to the nearest representable floating-point number."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# unable to represent, use default rounding\n",
      "@printf(\"%4.16lf\\n\", 1.2000000000000001)\n",
      "\n",
      "# rounding up\n",
      "with_rounding(Float64, RoundUp) do \n",
      "    @printf(\"%4.16lf\\n\", 1.1 + 0.1)\n",
      "end\n",
      "\n",
      "# rounding down\n",
      "with_rounding(Float64, RoundDown) do \n",
      "    @printf(\"%4.16lf\\n\", 1.1 + 0.1)\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For understanding of machine numbers, we take a look at IEEE standard for Single Precision(``Float32``), it consists of one bit of sign, and 8 bits of exponent bitstring, 23 bits of numerical value. Single Precision is ranged from ``-Inf32`` to ``Inf32``. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "- typemax(Float32) == typemin(Float32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "``Julia`` provides bits representation of a floating-point number by using ``bits``."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Float32 type 1.000001 is first rounded to 1.0000009536743164\n",
      "@printf(\"1.000001f0 is rounded to %4.16lf\\n\", 1.000001f0)\n",
      "\n",
      "# And then its bit representation as 0 01111111 00000000000000000001000\n",
      "@printf(\"its bits reprenstation is %s\\n\", bits(1.000001f0))\n",
      "\n",
      "# The representation means it is positive, exponent as 127(convert to base 10).\n",
      "# And numerical value part as 1 + 2^(-20). \n",
      "# Thus the answer will be (1 + 2^(-20))*2^(127 - 127)\n",
      "@printf(\"Convert to Float32 %4.16lf\\n\", float32(1 + 2.0^(-20)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So according to the IEEE standard, the ``Inf32`` is defined as ``0 11111111 00000000000000000000000``, if any of the last 23 bits is nonzero, then it is an ``NaN``, ``Julia`` sets its system ``NaN`` as ``0 11111111 10000000000000000000000``."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# bits of Inf32\n",
      "@printf(\"%s\\n\", bits(Inf32))\n",
      "# bits of NaN32\n",
      "@printf(\"%s\\n\", bits(NaN32))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Fixed Precision Arithmetic ##\n",
      "\n",
      "We are not going to talk about arbitrary-precision arithmetic here, there are many multiple-precision libraries nowadays. \n",
      "\n",
      "### Summation over array ###\n",
      "\n",
      "For simple loop like following would be a bad idea. We shall talk about the algorithm's accuracy as well as cost and stablity."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# simple loop of summation over array(double precision)\n",
      "function simple_sum(arr::AbstractArray, first::Int, last::Int)\n",
      "    b = arr[first];\n",
      "    for i = first + 1 : last\n",
      "        @inbounds b += arr[i]\n",
      "    end\n",
      "    return b\n",
      "end\n",
      "\n",
      "function pair_sum(arr::AbstractArray, first::Int, last::Int)\n",
      "    if first + 1024 >= last\n",
      "        return simple_sum(arr, first, last)\n",
      "    end\n",
      "    mid = (last + first) >>> 1;\n",
      "    return pair_sum(arr, first, mid) + pair_sum(arr, mid + 1, last)\n",
      "end\n",
      "\n",
      "function kahan_sum(arr::AbstractArray)\n",
      "    # preallocate memory\n",
      "    n = length(arr)\n",
      "    arr_i = 0.\n",
      "    if (n == 0)\n",
      "        return 0\n",
      "    end\n",
      "    @inbounds s = arr[1]\n",
      "    c = 0.\n",
      "    for i =  2:n\n",
      "        @inbounds arr_i = arr[i]\n",
      "        t = s + arr_i\n",
      "        if abs(s) >= abs(arr_i)\n",
      "            c += ((s-t) + arr_i)\n",
      "        else\n",
      "            c += ((arr_i-t) + s)\n",
      "        end\n",
      "        s = t\n",
      "    end\n",
      "    return s + c\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider the worst case of above routine ``simple_sum``, the accumulative error would grow as O(``eps`` n), where ``eps`` is the machine accuracy. On the other hand, the cost of computing is fine, O(n). However, we have other algorithms of summation, such as pairwise summation and Kahan summation. For pairwise summation, the algorithm is simply ``divide and conquer``, however, this reduces the growth of error to O(``eps`` log n), and for Kahan summation, it reduces the error to machine accuracy level as O(``eps``), however, this makes it slower than other methods.\n",
      "\n",
      "For the cost of each routine, Kahan costs most, the others are the same as O(n), but pairwise(cascade) summation can be parallelized. \n",
      "\n",
      "For the accuracy of each routine, we have to look deeper into this. For Kahan's method, since there is always compensation, we can always expect very accurate answer at each step in recursive summation, if performance is not our goal. However, for the simple summation method, the ordering of input array might play a great role in controlling the error.\n",
      "\n",
      "First, we have to think about why summation of harmonic series 'converges' on our fix-precision computer(maybe after a long time). When we are adding two numbers ``a + b``, if ``|a|`` is much larger than ``|b|``, then ``b`` probably could not contribute to the sum. Just like following case, ``10^(-15)`` is too small for ``log(10^15)``. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "log(10^15) + 0.000000000000001 - log(10^15) == 0.000000000000001"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Therefore, the ordering here really matters. If all numbers of the array ``A`` are postive, then asceding order will produce less error than other ordering, especially descending order. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rand_arr = rand(500000);\n",
      "\n",
      "# use ascending order will give better result\n",
      "sorted_rand_arr = sort(rand_arr,rev=false)\n",
      "\n",
      "# simple loop \n",
      "@printf(\"simple sum:\\n\")\n",
      "@time @inbounds ret_1 = simple_sum(rand_arr, 1, length(rand_arr))\n",
      "@time @inbounds sorted_ret_1 = simple_sum(sorted_rand_arr, 1, length(rand_arr))\n",
      "\n",
      "\n",
      "@printf(\"\\npairwise sum:\\n\")\n",
      "# pairwise/ uses simple loop for small block\n",
      "@time @inbounds ret_2 = pair_sum(rand_arr, 1, length(rand_arr))\n",
      "@time @inbounds sorted_ret_2 = pair_sum(sorted_rand_arr, 1, length(rand_arr))\n",
      "\n",
      "# Julia's mapreduce\n",
      "@printf(\"\\nJulia's mapreduce:\\n\")\n",
      "@time @inbounds ret_3 = sum(rand_arr)\n",
      "@time @inbounds sorted_ret_3 = sum(sorted_rand_arr)\n",
      "\n",
      "# Kahan \n",
      "@printf(\"\\nKahan sum:\\n\")\n",
      "@time @inbounds ret_4 = kahan_sum(rand_arr)\n",
      "@time @inbounds sorted_ret_4 = kahan_sum(sorted_rand_arr)\n",
      "\n",
      "@printf(\"\\nunsorted simple   sum is %4.16lf.\\n\",ret_1);\n",
      "@printf(\"\\nsorted simple     sum is %4.16lf.\\n\",sorted_ret_1);\n",
      "@printf(\"\\nunsorted pairwise sum is %4.16lf.\\n\",ret_2);\n",
      "@printf(\"\\nsorted pairwise   sum is %4.16lf.\\n\",sorted_ret_2);\n",
      "@printf(\"\\nunsorted Julia's  sum is %4.16lf.\\n\",ret_3);\n",
      "@printf(\"\\nsorted Julia's    sum is %4.16lf.\\n\",sorted_ret_3);\n",
      "@printf(\"\\nunsorted Kahan    sum is %4.16lf.\\n\",ret_4);\n",
      "@printf(\"\\nsorted Kahan      sum is %4.16lf.\\n\",sorted_ret_4);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Although it seems compensated method takes much longer time than other methods, we still have to use these methods for high accuracy computing, like long term integration, simple loop will explode our accuracy at some time, but Kahan will never ruin our result too much. Following example shows the integration of function ``sin(x)`` over ``[0, Pi]``, using simple loop and Kahan both, we can see the error of simple loop at the last case begins to raise, which would generate a U-shape plot, while Kahan makes the error continue to decrease."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Caution: this takes time\n",
      "f(x) = sin(x)\n",
      "\n",
      "@time for i = 1:8\n",
      "    SIZE = 10^i\n",
      "    nodes = zeros(SIZE)\n",
      "    \n",
      "    # most time spent on evaluating function as assignment\n",
      "    for j = 1:SIZE\n",
      "        @inbounds nodes[j] = sin(j*pi/SIZE)*pi/SIZE\n",
      "    end\n",
      "\n",
      "    ret_1 = simple_sum(nodes,1,SIZE)\n",
      "    ret_2 = kahan_sum(nodes)\n",
      "    @printf(\"%d    %4.16lf,        %4.16lf\\n\", i, log(abs(ret_1 - 2.0)), log(abs(ret_2 - 2.0)))\n",
      "end\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
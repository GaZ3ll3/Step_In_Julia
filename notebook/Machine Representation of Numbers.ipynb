{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:96b2ecf3e5834d01e2b05bb880592ed384c74906cb0392bd302ec134f352f7d2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Machine Representation of Numbers ##\n",
      "\n",
      "``Julia`` supports multiple float-point types, including ``Float16`` (half), ``Float32`` (single) and  ``Float64`` (double). Half-precision floating-point numbers are only for storage format, when ``Float16`` type is involved in computation, it will be automatically promoted into ``Float32``. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@printf(\"size of Float64 1.0 is %d.\\n\", sizeof(1.0))\n",
      "@printf(\"size of Float32 1.0 is %d.\\n\", sizeof(float32(1.0)))\n",
      "@printf(\"size of Float16 1.0 is %d.\\n\", sizeof(float16(1.0)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "``NaN`` and ``Inf`` (as well as ``-Inf``) are special floating-points(IEC559), and can be cast into all floating-point types also be used in arithmetic operations.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@printf(\"Inf * Inf + Inf = %d\\n\", Inf * Inf + Inf) \n",
      "@printf(\"Inf + (-Inf)    = %d\\n\", Inf + (-Inf))\n",
      "@printf(\"Inf * Inf + Inf = %lf\\n\", Inf * Inf + Inf) \n",
      "@printf(\"Inf + (-Inf)    = %lf\\n\", Inf + (-Inf))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Epsilon function ``eps`` gives machine accuracy. For single and double accuracy, epsilon will be ``float32(2.0^-23)`` and ``2.0^-53`` relatively. Notice ``eps`` also can be used on ``Float16``, the result will be also a half precision number.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@printf(\"%4.52lf\\n\", eps(Float16))\n",
      "@printf(\"%4.52lf\\n\" ,eps(Float32))\n",
      "@printf(\"%4.52lf\\n\", eps(Float64))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The default rounding mode is ``RoundNearest``, to change the mode, we can use ``with_rounding``. In the following example, ``1.2000000000000001`` cannot be represented, thus ``Julia`` rounds it to the nearest representable floating-point number."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# unable to represent, use default rounding\n",
      "@printf(\"%4.16lf\\n\", 1.2000000000000001)\n",
      "\n",
      "# rounding up\n",
      "with_rounding(Float64, RoundUp) do \n",
      "    @printf(\"%4.16lf\\n\", 1.1 + 0.1)\n",
      "end\n",
      "\n",
      "# rounding down\n",
      "with_rounding(Float64, RoundDown) do \n",
      "    @printf(\"%4.16lf\\n\", 1.1 + 0.1)\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For understanding of machine numbers, we take a look at IEEE standard for Single Precision(``Float32``), it consists of one bit of sign, and 8 bits of exponent bitstring, 23 bits of numerical value. Single Precision is ranged from ``-Inf32`` to ``Inf32``. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "- typemax(Float32) == typemin(Float32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "``Julia`` provides bits representation of a floating-point number by using ``bits``."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Float32 type 1.000001 is first rounded to 1.0000009536743164\n",
      "@printf(\"1.000001f0 is rounded to %4.16lf\\n\", 1.000001f0)\n",
      "\n",
      "# And then its bit representation as 0 01111111 00000000000000000001000\n",
      "@printf(\"its bits reprenstation is %s\\n\", bits(1.000001f0))\n",
      "\n",
      "# The representation means it is positive, exponent as 127(convert to base 10).\n",
      "# And numerical value part as 1 + 2^(-20). \n",
      "# Thus the answer will be (1 + 2^(-20))*2^(127 - 127)\n",
      "@printf(\"Convert to Float32 %4.16lf\\n\", float32(1 + 2.0^(-20)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So according to the IEEE standard, the ``Inf32`` is defined as ``0 11111111 00000000000000000000000``, if any of the last 23 bits is nonzero, then it is an ``NaN``, ``Julia`` sets its system ``NaN`` as ``0 11111111 10000000000000000000000``."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# bits of Inf32\n",
      "@printf(\"%s\\n\", bits(Inf32))\n",
      "# bits of NaN32\n",
      "@printf(\"%s\\n\", bits(NaN32))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
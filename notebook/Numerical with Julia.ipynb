{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:2dd0a621ed7c92f4e01fcf4bbc7aee0bec305a46b21e37bc655d209b6c117acc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Numerical with Julia #\n",
      "\n",
      "This tutorial is not for Julia beginners.\n",
      "\n",
      "## Machine Representation of Numbers ##\n",
      "\n",
      "``Julia`` supports multiple float-point types, including ``Float16`` (half), ``Float32`` (single) and  ``Float64`` (double). Half-precision floating-point numbers are only for storage format, when ``Float16`` type is involved in computation, it will be automatically promoted into ``Float32``. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@printf(\"size of Float64 1.0 is %d.\\n\", sizeof(1.0))\n",
      "@printf(\"size of Float32 1.0 is %d.\\n\", sizeof(float32(1.0)))\n",
      "@printf(\"size of Float16 1.0 is %d.\\n\", sizeof(float16(1.0)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "``NaN`` and ``Inf`` (as well as ``-Inf``) are special floating-points(IEC559), and can be cast into all floating-point types also be used in arithmetic operations.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@printf(\"Inf * Inf + Inf = %d\\n\", Inf * Inf + Inf) \n",
      "@printf(\"Inf + (-Inf)    = %d\\n\", Inf + (-Inf))\n",
      "@printf(\"Inf * Inf + Inf = %lf\\n\", Inf * Inf + Inf) \n",
      "@printf(\"Inf + (-Inf)    = %lf\\n\", Inf + (-Inf))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Epsilon function ``eps`` gives machine accuracy. For single and double accuracy, epsilon will be ``float32(2.0^-23)`` and ``2.0^-53`` relatively. Notice ``eps`` also can be used on ``Float16``, the result will be also a half precision number.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@printf(\"%4.52lf\\n\", eps(Float16))\n",
      "@printf(\"%4.52lf\\n\" ,eps(Float32))\n",
      "@printf(\"%4.52lf\\n\", eps(Float64))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The default rounding mode is ``RoundNearest``, to change the mode, we can use ``with_rounding``. In the following example, ``1.2000000000000001`` cannot be represented, thus ``Julia`` rounds it to the nearest representable floating-point number."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# unable to represent, use default rounding\n",
      "@printf(\"%4.16lf\\n\", 1.2000000000000001)\n",
      "\n",
      "# rounding up\n",
      "with_rounding(Float64, RoundUp) do \n",
      "    @printf(\"%4.16lf\\n\", 1.1 + 0.1)\n",
      "end\n",
      "\n",
      "# rounding down\n",
      "with_rounding(Float64, RoundDown) do \n",
      "    @printf(\"%4.16lf\\n\", 1.1 + 0.1)\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Linear Algebra ##\n",
      "\n",
      "Linear algebra functions in ``Julia`` are largely implemented by calling functions from [LAPACK](http://www.netlib.org/lapack). Sparse factorizations call functions from SuiteSparse.\n",
      "\n",
      "### Matrices factorization ###\n",
      "\n",
      "\n",
      "For factorization algorithms, ``Julia`` has already implemented common routines as ``lu``, ``qr`` and ``chol``. The routines call ``LAPACK`` subroutines for decomposition algorithm in general. For example, ``lufact!`` calls ``LAPACK.getrf!`` to update matrices in place.\n",
      "\n",
      "``` julia\n",
      "    function lufact!{T<:BlasFloat}(A::StridedMatrix{T};  pivot = true)\n",
      "        !pivot && return generic_lufact!(A,pivot = pivot)\n",
      "        lpt =  LAPACK.getrf!(A)\n",
      "        return LU{T, typeof(A)}(lpt[1], lpt[2], lpt[3])\n",
      "```\n",
      "And ``LAPACK.getrf!`` calls ``getrf`` against the ``liblapack``.\n",
      "\n",
      "``` \n",
      "    ccall(($(string(getrf)), liblapack), Void, Ptr{BlasInt}, \n",
      "        Ptr{BlasInt}, Ptr{$elty}, Ptr{BlasInt}, Ptr{BlasInt},\n",
      "        Ptr{BlasInt}, &m, &n, A, &lda, ipiv, info)\n",
      "```\n",
      "\n",
      "### Eigenvalues & Singular Value ###\n",
      "\n",
      "The most common routines ``eig`` and ``svd`` call ``LAPACK.geevx!`` and ``LAPACK.gesdd!`` relatively. \n",
      "\n",
      "``svds`` does not exist but can be found with external package -- ``IterativeSolvers.jl``.\n",
      "\n",
      "#### Solve ``Ax == b`` ####\n",
      "\n",
      "Solving linear system involves [direct method](http://en.wikipedia.org/wiki/direct_method) and [iterative method](http://en.wikipedia.org/wiki/iterative_method).\n",
      "\n",
      "``Julia`` has builtin function ``\\(A,B)`` for solving linear system, which calls ``UMFPACK`` solver.\n",
      "\n",
      "For iterative methods, there are plenty of routines(preconditioned) to choose from, such as ``CG``, ``GMRES``, ``SOR``, ``SSOR``, ``Lanczos``.\n",
      "\n",
      "The above routines can be found through ``IterativeSolvers.jl`` for ``0.2.1-``. In this repository, we store our ``0.3.0+`` compatible routines in ``src`` directory, for example, following routines are stored in ``src/linalg/iterative``.  \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Optimization ###\n",
      "\n",
      "``JuliaOpt`` has built pure ``Julia`` package ``Optim`` for general optimization algorithms. Another package ``JuMP`` involves linear- and quadratic-constrained problems, which supports [Ipopt](https://projects.coin-or.org/Ipopt) through wrapper ``Ipopt.jl`` and many other solvers -- ``Cbc``, ``MOSEK``, ``Gurobi``, etc.\n",
      "\n",
      "\n",
      "\n",
      "### Approximation & Differentiation ###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Quadrature ###\n",
      "\n",
      "``Julia`` has builtin support for one-dimensional integration as ``quadgk``, using adaptive *Gauss-Kronrod* algorithm. \n",
      "\n",
      "\n",
      "``` julia\n",
      "    function quadgk{T<:FloatingPoint}(f, a::Complex{T},\n",
      "                                      b::Complex{T}, c::Complex{T}...,\n",
      "                                      abstol=zero(T), reltol=sqrt(eps(T))\n",
      "                                      maxevals=10^7, order=7,norm=vecnorm)\n",
      "        do_quadgk(f, [a,b,c...], order, T, abstol, reltol, maxevals, norm)\n",
      "    end\n",
      "```\n",
      "For multi-dimensional case(including one dimensional), external package ``Cubature.jl`` can handle integrand over multi-dimensional box. \n",
      "\n",
      "In ``/src/integr``, we made more quadrature functions for use.\n",
      "\n",
      "\n",
      "\n",
      "### Ordinary Differential Equation ###\n",
      "\n",
      "\n",
      "\n",
      "### Partial Differential Equation ###\n",
      "\n",
      "For solving partial differential equations, numerically we have lots of choices from various schemes on finite difference method or finite volume and finite element method. However, right now at ``prerelease 0.3.0``, there is no public packages on this. And integration of ``PETsc`` into ``Julia`` is absolutely a clever way to reuse the highly optimized and parallelized codes, again, there is no easy job.\n",
      " \n",
      "\n",
      "## Symbolic Computation ##"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}